{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, sys, time, random, math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from typing import Optional, List\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, size = 224, center_crop = True):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = df['file_name'].tolist()\n",
    "        self.findings = df['text'].tolist()\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.image_transforms = transforms.ToTensor()\n",
    "        # transforms.Compose(\n",
    "        #     [\n",
    "        #         transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        #         transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize([0.5], [0.5]),\n",
    "        #     ]\n",
    "        # )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = {}\n",
    "        instance_image = Image.open(\n",
    "            os.path.join(self.root_dir, self.files[idx])\n",
    "        ).convert(\"RGB\")\n",
    "\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "        example[\"instance_prompt_ids\"] = self.findings[idx]\n",
    "        # self.tokenizer(\n",
    "        #     self.findings[idx],\n",
    "        #     truncation=True,\n",
    "        #     padding=\"max_length\",\n",
    "        #     max_length=self.tokenizer.model_max_length,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # ).input_ids\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main path\n",
    "Main_Path = '/home/mcrespo/migros_deepL'\n",
    "# The datasets path under the main path\n",
    "Data_storage = Main_Path + '/sample_flair'\n",
    "save_result_path = Main_Path + '/selora_outputs'\n",
    "reports_path = Data_storage + '/metadata.csv'\n",
    "### folder to save the result.\n",
    "folder_name = 'loras'\n",
    "\n",
    "metadata = pd.read_csv(reports_path)\n",
    "train_df, temp_df = train_test_split(metadata, test_size=0.2, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_ds = ImageDataset(\n",
    "    root_dir=Data_storage,\n",
    "    df=train_df\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers = 0)\n",
    "### batch size determines the number of steps for each epoch, we are doing 100 epochs. So total number of steps is : 100 * train_df//batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "model_unet = '/work/scratch/mcrespo/output/test_12_26_alldataset/12-29_17h45m15s/selora_outputs/loras/trained_model/final_Unet/diffusion_pytorch_model.safetensors'\n",
    "model_text= '/work/scratch/mcrespo/output/test_12_26_alldataset/12-29_17h45m15s/selora_outputs/loras/trained_model/final_Text/model.safetensors'\n",
    "\n",
    "model_text = load_file(model_text)\n",
    "model_unet = load_file(model_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import safetensors.torch\n",
    "\n",
    "def merge_weights(tensor_dict):\n",
    "    \"\"\"\n",
    "    Merges the weights for layers containing weight, lora_A, lora_B according to W' = W + AB^T.\n",
    "\n",
    "    Args:\n",
    "        tensor_dict (dict): Dictionary containing the tensor weights.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated tensor dictionary with merged weights.\n",
    "    \"\"\"\n",
    "    merged_tensors = {}\n",
    "\n",
    "    for key, tensor in tensor_dict.items():\n",
    "        # Check if this is a weight tensor\n",
    "        if \"weight\" in key and \"lora_A\" not in key and \"lora_B\" not in key:\n",
    "            base_key = key.replace(\".weight\", \"\")\n",
    "\n",
    "            lora_A_key = base_key + \".lora_A\"\n",
    "            lora_B_key = base_key + \".lora_B\"\n",
    "\n",
    "            if lora_A_key in tensor_dict and lora_B_key in tensor_dict:\n",
    "                A = tensor_dict[lora_A_key]\n",
    "                B = tensor_dict[lora_B_key]\n",
    "                # Merge weights: W' = W + AB^T\n",
    "                # (self.lora_B @ self.lora_A)\n",
    "\n",
    "                merged_tensor = tensor + (B @ A)\n",
    "\n",
    "                merged_tensors[base_key + \".weight\"] = merged_tensor\n",
    "            else:\n",
    "                # Keep the original weight if no lora tensors are found\n",
    "                merged_tensors[key] = tensor\n",
    "                \n",
    "        elif \"lora_A\" not in key and \"lora_B\" not in key:\n",
    "        #     # Copy other tensors as is\n",
    "            merged_tensors[key] = tensor\n",
    "\n",
    "    return merged_tensors\n",
    "\n",
    "\n",
    "# Merge the weights\n",
    "merged_text_encoder = merge_weights(model_text)\n",
    "merged_unet = merge_weights(model_unet)\n",
    "# # Save the merged tensors back to a safetensors file\n",
    "# output_path = \"merged_file.safetensors\"\n",
    "# safetensors.torch.save_file(merged_tensor_dict, output_path)\n",
    "\n",
    "# print(f\"Merged weights saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/work/scratch/mcrespo/output/test_12_26_alldataset/12-29_17h45m15s/selora_outputs/loras/trained_model/'\n",
    "safetensors.torch.save_file(merged_text_encoder, output_path + 'final_Text/merged_model.safetensors')\n",
    "safetensors.torch.save_file(merged_unet, output_path + 'final_Unet/merged_model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_in.bias\n",
      "torch.Size([320])\n",
      "conv_in.weight\n",
      "torch.Size([320, 4, 3, 3])\n",
      "conv_norm_out.bias\n",
      "torch.Size([320])\n",
      "conv_norm_out.weight\n",
      "torch.Size([320])\n",
      "conv_out.bias\n",
      "torch.Size([4])\n",
      "conv_out.weight\n",
      "torch.Size([4, 320, 3, 3])\n",
      "down_blocks.0.attentions.0.norm.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.norm.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.proj_in.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.proj_in.weight\n",
      "torch.Size([320, 320, 1, 1])\n",
      "down_blocks.0.attentions.0.proj_out.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.proj_out.weight\n",
      "torch.Size([320, 320, 1, 1])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([320, 768])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([320, 768])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([2560])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([2560, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([320, 1280])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.norm.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.norm.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.proj_in.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.proj_in.weight\n",
      "torch.Size([320, 320, 1, 1])\n",
      "down_blocks.0.attentions.1.proj_out.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.proj_out.weight\n",
      "torch.Size([320, 320, 1, 1])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([320, 768])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([320, 768])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([2560])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([2560, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([320, 1280])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.downsamplers.0.conv.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.downsamplers.0.conv.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "down_blocks.0.resnets.0.conv1.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.0.conv1.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "down_blocks.0.resnets.0.conv2.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.0.conv2.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "down_blocks.0.resnets.0.norm1.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.0.norm1.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.0.norm2.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.0.norm2.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.0.time_emb_proj.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.0.time_emb_proj.weight\n",
      "torch.Size([320, 1280])\n",
      "down_blocks.0.resnets.1.conv1.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.1.conv1.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "down_blocks.0.resnets.1.conv2.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.1.conv2.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "down_blocks.0.resnets.1.norm1.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.1.norm1.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.1.norm2.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.1.norm2.weight\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.1.time_emb_proj.bias\n",
      "torch.Size([320])\n",
      "down_blocks.0.resnets.1.time_emb_proj.weight\n",
      "torch.Size([320, 1280])\n",
      "down_blocks.1.attentions.0.norm.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.norm.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.proj_in.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.proj_in.weight\n",
      "torch.Size([640, 640, 1, 1])\n",
      "down_blocks.1.attentions.0.proj_out.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.proj_out.weight\n",
      "torch.Size([640, 640, 1, 1])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([640, 768])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([640, 768])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([5120])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([5120, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([640, 2560])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.norm.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.norm.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.proj_in.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.proj_in.weight\n",
      "torch.Size([640, 640, 1, 1])\n",
      "down_blocks.1.attentions.1.proj_out.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.proj_out.weight\n",
      "torch.Size([640, 640, 1, 1])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([640, 768])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([640, 768])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([5120])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([5120, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([640, 2560])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.downsamplers.0.conv.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.downsamplers.0.conv.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "down_blocks.1.resnets.0.conv1.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.0.conv1.weight\n",
      "torch.Size([640, 320, 3, 3])\n",
      "down_blocks.1.resnets.0.conv2.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.0.conv2.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "down_blocks.1.resnets.0.conv_shortcut.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.0.conv_shortcut.weight\n",
      "torch.Size([640, 320, 1, 1])\n",
      "down_blocks.1.resnets.0.norm1.bias\n",
      "torch.Size([320])\n",
      "down_blocks.1.resnets.0.norm1.weight\n",
      "torch.Size([320])\n",
      "down_blocks.1.resnets.0.norm2.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.0.norm2.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.0.time_emb_proj.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.0.time_emb_proj.weight\n",
      "torch.Size([640, 1280])\n",
      "down_blocks.1.resnets.1.conv1.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.1.conv1.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "down_blocks.1.resnets.1.conv2.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.1.conv2.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "down_blocks.1.resnets.1.norm1.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.1.norm1.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.1.norm2.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.1.norm2.weight\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.1.time_emb_proj.bias\n",
      "torch.Size([640])\n",
      "down_blocks.1.resnets.1.time_emb_proj.weight\n",
      "torch.Size([640, 1280])\n",
      "down_blocks.2.attentions.0.norm.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.norm.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.proj_in.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.proj_in.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "down_blocks.2.attentions.0.proj_out.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.proj_out.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([10240])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([10240, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([1280, 5120])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.norm.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.norm.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.proj_in.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.proj_in.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "down_blocks.2.attentions.1.proj_out.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.proj_out.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([10240])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([10240, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([1280, 5120])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.downsamplers.0.conv.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.downsamplers.0.conv.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.2.resnets.0.conv1.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.0.conv1.weight\n",
      "torch.Size([1280, 640, 3, 3])\n",
      "down_blocks.2.resnets.0.conv2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.0.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.2.resnets.0.conv_shortcut.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.0.conv_shortcut.weight\n",
      "torch.Size([1280, 640, 1, 1])\n",
      "down_blocks.2.resnets.0.norm1.bias\n",
      "torch.Size([640])\n",
      "down_blocks.2.resnets.0.norm1.weight\n",
      "torch.Size([640])\n",
      "down_blocks.2.resnets.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.0.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.0.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.2.resnets.1.conv1.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.1.conv1.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.2.resnets.1.conv2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.1.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.2.resnets.1.norm1.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.1.norm1.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.1.norm2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.1.norm2.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.1.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.2.resnets.1.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.3.resnets.0.conv1.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.0.conv1.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.3.resnets.0.conv2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.0.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.3.resnets.0.norm1.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.0.norm1.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.0.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.0.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "down_blocks.3.resnets.1.conv1.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.1.conv1.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.3.resnets.1.conv2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.1.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.3.resnets.1.norm1.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.1.norm1.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.1.norm2.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.1.norm2.weight\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.1.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "down_blocks.3.resnets.1.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.norm.bias\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.norm.weight\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.proj_in.bias\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.proj_in.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "mid_block.attentions.0.proj_out.bias\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.proj_out.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([1280, 768])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([1280, 768])\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([10240])\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([10240, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([1280, 5120])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.bias\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.weight\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.bias\n",
      "torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.weight\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.0.conv1.bias\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.0.conv1.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "mid_block.resnets.0.conv2.bias\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.0.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "mid_block.resnets.0.norm1.bias\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.0.norm1.weight\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.0.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.0.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "mid_block.resnets.1.conv1.bias\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.1.conv1.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "mid_block.resnets.1.conv2.bias\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.1.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "mid_block.resnets.1.norm1.bias\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.1.norm1.weight\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.1.norm2.bias\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.1.norm2.weight\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.1.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "mid_block.resnets.1.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "time_embedding.linear_1.bias\n",
      "torch.Size([1280])\n",
      "time_embedding.linear_1.weight\n",
      "torch.Size([1280, 320])\n",
      "time_embedding.linear_2.bias\n",
      "torch.Size([1280])\n",
      "time_embedding.linear_2.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.0.resnets.0.conv1.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.0.conv1.weight\n",
      "torch.Size([1280, 2560, 3, 3])\n",
      "up_blocks.0.resnets.0.conv2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.0.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.0.resnets.0.conv_shortcut.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.0.conv_shortcut.weight\n",
      "torch.Size([1280, 2560, 1, 1])\n",
      "up_blocks.0.resnets.0.norm1.bias\n",
      "torch.Size([2560])\n",
      "up_blocks.0.resnets.0.norm1.weight\n",
      "torch.Size([2560])\n",
      "up_blocks.0.resnets.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.0.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.0.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.0.resnets.1.conv1.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.1.conv1.weight\n",
      "torch.Size([1280, 2560, 3, 3])\n",
      "up_blocks.0.resnets.1.conv2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.1.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.0.resnets.1.conv_shortcut.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.1.conv_shortcut.weight\n",
      "torch.Size([1280, 2560, 1, 1])\n",
      "up_blocks.0.resnets.1.norm1.bias\n",
      "torch.Size([2560])\n",
      "up_blocks.0.resnets.1.norm1.weight\n",
      "torch.Size([2560])\n",
      "up_blocks.0.resnets.1.norm2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.1.norm2.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.1.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.1.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.0.resnets.2.conv1.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.2.conv1.weight\n",
      "torch.Size([1280, 2560, 3, 3])\n",
      "up_blocks.0.resnets.2.conv2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.2.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.0.resnets.2.conv_shortcut.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.2.conv_shortcut.weight\n",
      "torch.Size([1280, 2560, 1, 1])\n",
      "up_blocks.0.resnets.2.norm1.bias\n",
      "torch.Size([2560])\n",
      "up_blocks.0.resnets.2.norm1.weight\n",
      "torch.Size([2560])\n",
      "up_blocks.0.resnets.2.norm2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.2.norm2.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.2.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.resnets.2.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.0.upsamplers.0.conv.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.0.upsamplers.0.conv.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.1.attentions.0.norm.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.norm.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.proj_in.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.proj_in.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.0.proj_out.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.proj_out.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([10240])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([10240, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([1280, 5120])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.norm.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.norm.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.proj_in.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.proj_in.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.1.proj_out.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.proj_out.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([10240])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([10240, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([1280, 5120])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.norm.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.norm.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.proj_in.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.proj_in.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.2.proj_out.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.proj_out.weight\n",
      "torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([10240])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([10240, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([1280, 5120])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.0.conv1.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.0.conv1.weight\n",
      "torch.Size([1280, 2560, 3, 3])\n",
      "up_blocks.1.resnets.0.conv2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.0.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.1.resnets.0.conv_shortcut.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.0.conv_shortcut.weight\n",
      "torch.Size([1280, 2560, 1, 1])\n",
      "up_blocks.1.resnets.0.norm1.bias\n",
      "torch.Size([2560])\n",
      "up_blocks.1.resnets.0.norm1.weight\n",
      "torch.Size([2560])\n",
      "up_blocks.1.resnets.0.norm2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.0.norm2.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.0.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.0.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.resnets.1.conv1.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.1.conv1.weight\n",
      "torch.Size([1280, 2560, 3, 3])\n",
      "up_blocks.1.resnets.1.conv2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.1.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.1.resnets.1.conv_shortcut.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.1.conv_shortcut.weight\n",
      "torch.Size([1280, 2560, 1, 1])\n",
      "up_blocks.1.resnets.1.norm1.bias\n",
      "torch.Size([2560])\n",
      "up_blocks.1.resnets.1.norm1.weight\n",
      "torch.Size([2560])\n",
      "up_blocks.1.resnets.1.norm2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.1.norm2.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.1.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.1.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.resnets.2.conv1.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.2.conv1.weight\n",
      "torch.Size([1280, 1920, 3, 3])\n",
      "up_blocks.1.resnets.2.conv2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.2.conv2.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.1.resnets.2.conv_shortcut.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.2.conv_shortcut.weight\n",
      "torch.Size([1280, 1920, 1, 1])\n",
      "up_blocks.1.resnets.2.norm1.bias\n",
      "torch.Size([1920])\n",
      "up_blocks.1.resnets.2.norm1.weight\n",
      "torch.Size([1920])\n",
      "up_blocks.1.resnets.2.norm2.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.2.norm2.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.2.time_emb_proj.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.resnets.2.time_emb_proj.weight\n",
      "torch.Size([1280, 1280])\n",
      "up_blocks.1.upsamplers.0.conv.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.1.upsamplers.0.conv.weight\n",
      "torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.2.attentions.0.norm.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.norm.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.proj_in.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.proj_in.weight\n",
      "torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.0.proj_out.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.proj_out.weight\n",
      "torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([640, 768])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([640, 768])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([5120])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([5120, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([640, 2560])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.norm.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.norm.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.proj_in.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.proj_in.weight\n",
      "torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.1.proj_out.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.proj_out.weight\n",
      "torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([640, 768])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([640, 768])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([5120])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([5120, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([640, 2560])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.norm.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.norm.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.proj_in.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.proj_in.weight\n",
      "torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.2.proj_out.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.proj_out.weight\n",
      "torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([640, 768])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([640, 768])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([5120])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([5120, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([640, 2560])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.0.conv1.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.0.conv1.weight\n",
      "torch.Size([640, 1920, 3, 3])\n",
      "up_blocks.2.resnets.0.conv2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.0.conv2.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "up_blocks.2.resnets.0.conv_shortcut.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.0.conv_shortcut.weight\n",
      "torch.Size([640, 1920, 1, 1])\n",
      "up_blocks.2.resnets.0.norm1.bias\n",
      "torch.Size([1920])\n",
      "up_blocks.2.resnets.0.norm1.weight\n",
      "torch.Size([1920])\n",
      "up_blocks.2.resnets.0.norm2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.0.norm2.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.0.time_emb_proj.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.0.time_emb_proj.weight\n",
      "torch.Size([640, 1280])\n",
      "up_blocks.2.resnets.1.conv1.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.1.conv1.weight\n",
      "torch.Size([640, 1280, 3, 3])\n",
      "up_blocks.2.resnets.1.conv2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.1.conv2.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "up_blocks.2.resnets.1.conv_shortcut.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.1.conv_shortcut.weight\n",
      "torch.Size([640, 1280, 1, 1])\n",
      "up_blocks.2.resnets.1.norm1.bias\n",
      "torch.Size([1280])\n",
      "up_blocks.2.resnets.1.norm1.weight\n",
      "torch.Size([1280])\n",
      "up_blocks.2.resnets.1.norm2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.1.norm2.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.1.time_emb_proj.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.1.time_emb_proj.weight\n",
      "torch.Size([640, 1280])\n",
      "up_blocks.2.resnets.2.conv1.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.2.conv1.weight\n",
      "torch.Size([640, 960, 3, 3])\n",
      "up_blocks.2.resnets.2.conv2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.2.conv2.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "up_blocks.2.resnets.2.conv_shortcut.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.2.conv_shortcut.weight\n",
      "torch.Size([640, 960, 1, 1])\n",
      "up_blocks.2.resnets.2.norm1.bias\n",
      "torch.Size([960])\n",
      "up_blocks.2.resnets.2.norm1.weight\n",
      "torch.Size([960])\n",
      "up_blocks.2.resnets.2.norm2.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.2.norm2.weight\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.2.time_emb_proj.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.resnets.2.time_emb_proj.weight\n",
      "torch.Size([640, 1280])\n",
      "up_blocks.2.upsamplers.0.conv.bias\n",
      "torch.Size([640])\n",
      "up_blocks.2.upsamplers.0.conv.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "up_blocks.3.attentions.0.norm.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.norm.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.proj_in.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.proj_in.weight\n",
      "torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.0.proj_out.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.proj_out.weight\n",
      "torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([320, 768])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([320, 768])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([2560])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([2560, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([320, 1280])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.norm.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.norm.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.proj_in.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.proj_in.weight\n",
      "torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.1.proj_out.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.proj_out.weight\n",
      "torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([320, 768])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([320, 768])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([2560])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([2560, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([320, 1280])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.norm.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.norm.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.proj_in.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.proj_in.weight\n",
      "torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.2.proj_out.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.proj_out.weight\n",
      "torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "torch.Size([320, 768])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "torch.Size([320, 768])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "torch.Size([2560])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "torch.Size([2560, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "torch.Size([320, 1280])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.0.conv1.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.0.conv1.weight\n",
      "torch.Size([320, 960, 3, 3])\n",
      "up_blocks.3.resnets.0.conv2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.0.conv2.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "up_blocks.3.resnets.0.conv_shortcut.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.0.conv_shortcut.weight\n",
      "torch.Size([320, 960, 1, 1])\n",
      "up_blocks.3.resnets.0.norm1.bias\n",
      "torch.Size([960])\n",
      "up_blocks.3.resnets.0.norm1.weight\n",
      "torch.Size([960])\n",
      "up_blocks.3.resnets.0.norm2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.0.norm2.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.0.time_emb_proj.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.0.time_emb_proj.weight\n",
      "torch.Size([320, 1280])\n",
      "up_blocks.3.resnets.1.conv1.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.1.conv1.weight\n",
      "torch.Size([320, 640, 3, 3])\n",
      "up_blocks.3.resnets.1.conv2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.1.conv2.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "up_blocks.3.resnets.1.conv_shortcut.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.1.conv_shortcut.weight\n",
      "torch.Size([320, 640, 1, 1])\n",
      "up_blocks.3.resnets.1.norm1.bias\n",
      "torch.Size([640])\n",
      "up_blocks.3.resnets.1.norm1.weight\n",
      "torch.Size([640])\n",
      "up_blocks.3.resnets.1.norm2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.1.norm2.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.1.time_emb_proj.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.1.time_emb_proj.weight\n",
      "torch.Size([320, 1280])\n",
      "up_blocks.3.resnets.2.conv1.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.2.conv1.weight\n",
      "torch.Size([320, 640, 3, 3])\n",
      "up_blocks.3.resnets.2.conv2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.2.conv2.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "up_blocks.3.resnets.2.conv_shortcut.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.2.conv_shortcut.weight\n",
      "torch.Size([320, 640, 1, 1])\n",
      "up_blocks.3.resnets.2.norm1.bias\n",
      "torch.Size([640])\n",
      "up_blocks.3.resnets.2.norm1.weight\n",
      "torch.Size([640])\n",
      "up_blocks.3.resnets.2.norm2.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.2.norm2.weight\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.2.time_emb_proj.bias\n",
      "torch.Size([320])\n",
      "up_blocks.3.resnets.2.time_emb_proj.weight\n",
      "torch.Size([320, 1280])\n"
     ]
    }
   ],
   "source": [
    "for name, tensor in merged_unet.items():\n",
    "    print(name)\n",
    "    print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_model.embeddings.position_embedding.weight\n",
      "torch.Size([77, 768])\n",
      "text_model.embeddings.token_embedding.weight\n",
      "torch.Size([49408, 768])\n",
      "text_model.encoder.layers.0.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.0.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.0.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.0.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.0.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.0.mlp.fc1.lora_A\n",
      "torch.Size([21, 768])\n",
      "text_model.encoder.layers.0.mlp.fc1.lora_A_temp\n",
      "torch.Size([22, 768])\n",
      "text_model.encoder.layers.0.mlp.fc1.lora_B\n",
      "torch.Size([3072, 21])\n",
      "text_model.encoder.layers.0.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 22])\n",
      "text_model.encoder.layers.0.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.0.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.0.mlp.fc2.lora_A\n",
      "torch.Size([1, 3072])\n",
      "text_model.encoder.layers.0.mlp.fc2.lora_A_temp\n",
      "torch.Size([2, 3072])\n",
      "text_model.encoder.layers.0.mlp.fc2.lora_B\n",
      "torch.Size([768, 1])\n",
      "text_model.encoder.layers.0.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.0.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.0.self_attn.k_proj.lora_A\n",
      "torch.Size([18, 768])\n",
      "text_model.encoder.layers.0.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([19, 768])\n",
      "text_model.encoder.layers.0.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 18])\n",
      "text_model.encoder.layers.0.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 19])\n",
      "text_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.0.self_attn.out_proj.lora_A\n",
      "torch.Size([24, 768])\n",
      "text_model.encoder.layers.0.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([25, 768])\n",
      "text_model.encoder.layers.0.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 24])\n",
      "text_model.encoder.layers.0.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 25])\n",
      "text_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.0.self_attn.q_proj.lora_A\n",
      "torch.Size([23, 768])\n",
      "text_model.encoder.layers.0.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([24, 768])\n",
      "text_model.encoder.layers.0.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 23])\n",
      "text_model.encoder.layers.0.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 24])\n",
      "text_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.0.self_attn.v_proj.lora_A\n",
      "torch.Size([22, 768])\n",
      "text_model.encoder.layers.0.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([23, 768])\n",
      "text_model.encoder.layers.0.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 22])\n",
      "text_model.encoder.layers.0.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 23])\n",
      "text_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.1.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.1.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.1.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.1.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.1.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.1.mlp.fc1.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.1.mlp.fc1.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.1.mlp.fc1.lora_B\n",
      "torch.Size([3072, 1])\n",
      "text_model.encoder.layers.1.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 2])\n",
      "text_model.encoder.layers.1.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.1.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.1.mlp.fc2.lora_A\n",
      "torch.Size([7, 3072])\n",
      "text_model.encoder.layers.1.mlp.fc2.lora_A_temp\n",
      "torch.Size([8, 3072])\n",
      "text_model.encoder.layers.1.mlp.fc2.lora_B\n",
      "torch.Size([768, 7])\n",
      "text_model.encoder.layers.1.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 8])\n",
      "text_model.encoder.layers.1.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.1.self_attn.k_proj.lora_A\n",
      "torch.Size([18, 768])\n",
      "text_model.encoder.layers.1.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([19, 768])\n",
      "text_model.encoder.layers.1.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 18])\n",
      "text_model.encoder.layers.1.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 19])\n",
      "text_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.1.self_attn.out_proj.lora_A\n",
      "torch.Size([23, 768])\n",
      "text_model.encoder.layers.1.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([24, 768])\n",
      "text_model.encoder.layers.1.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 23])\n",
      "text_model.encoder.layers.1.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 24])\n",
      "text_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.1.self_attn.q_proj.lora_A\n",
      "torch.Size([16, 768])\n",
      "text_model.encoder.layers.1.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([17, 768])\n",
      "text_model.encoder.layers.1.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 16])\n",
      "text_model.encoder.layers.1.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 17])\n",
      "text_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.1.self_attn.v_proj.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.1.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.1.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 1])\n",
      "text_model.encoder.layers.1.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.10.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.10.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.10.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.10.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.10.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.10.mlp.fc1.lora_A\n",
      "torch.Size([7, 768])\n",
      "text_model.encoder.layers.10.mlp.fc1.lora_A_temp\n",
      "torch.Size([8, 768])\n",
      "text_model.encoder.layers.10.mlp.fc1.lora_B\n",
      "torch.Size([3072, 7])\n",
      "text_model.encoder.layers.10.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 8])\n",
      "text_model.encoder.layers.10.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.10.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.10.mlp.fc2.lora_A\n",
      "torch.Size([2, 3072])\n",
      "text_model.encoder.layers.10.mlp.fc2.lora_A_temp\n",
      "torch.Size([3, 3072])\n",
      "text_model.encoder.layers.10.mlp.fc2.lora_B\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.10.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 3])\n",
      "text_model.encoder.layers.10.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.10.self_attn.k_proj.lora_A\n",
      "torch.Size([18, 768])\n",
      "text_model.encoder.layers.10.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([19, 768])\n",
      "text_model.encoder.layers.10.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 18])\n",
      "text_model.encoder.layers.10.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 19])\n",
      "text_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.10.self_attn.out_proj.lora_A\n",
      "torch.Size([5, 768])\n",
      "text_model.encoder.layers.10.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([6, 768])\n",
      "text_model.encoder.layers.10.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 5])\n",
      "text_model.encoder.layers.10.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 6])\n",
      "text_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.10.self_attn.q_proj.lora_A\n",
      "torch.Size([13, 768])\n",
      "text_model.encoder.layers.10.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([14, 768])\n",
      "text_model.encoder.layers.10.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 13])\n",
      "text_model.encoder.layers.10.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 14])\n",
      "text_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.10.self_attn.v_proj.lora_A\n",
      "torch.Size([13, 768])\n",
      "text_model.encoder.layers.10.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([14, 768])\n",
      "text_model.encoder.layers.10.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 13])\n",
      "text_model.encoder.layers.10.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 14])\n",
      "text_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.11.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.11.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.11.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.11.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.11.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.11.mlp.fc1.lora_A\n",
      "torch.Size([7, 768])\n",
      "text_model.encoder.layers.11.mlp.fc1.lora_A_temp\n",
      "torch.Size([8, 768])\n",
      "text_model.encoder.layers.11.mlp.fc1.lora_B\n",
      "torch.Size([3072, 7])\n",
      "text_model.encoder.layers.11.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 8])\n",
      "text_model.encoder.layers.11.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.11.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.11.mlp.fc2.lora_A\n",
      "torch.Size([3, 3072])\n",
      "text_model.encoder.layers.11.mlp.fc2.lora_A_temp\n",
      "torch.Size([4, 3072])\n",
      "text_model.encoder.layers.11.mlp.fc2.lora_B\n",
      "torch.Size([768, 3])\n",
      "text_model.encoder.layers.11.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 4])\n",
      "text_model.encoder.layers.11.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.11.self_attn.k_proj.lora_A\n",
      "torch.Size([16, 768])\n",
      "text_model.encoder.layers.11.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([17, 768])\n",
      "text_model.encoder.layers.11.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 16])\n",
      "text_model.encoder.layers.11.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 17])\n",
      "text_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.11.self_attn.out_proj.lora_A\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.11.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([3, 768])\n",
      "text_model.encoder.layers.11.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.11.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 3])\n",
      "text_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.11.self_attn.q_proj.lora_A\n",
      "torch.Size([11, 768])\n",
      "text_model.encoder.layers.11.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([12, 768])\n",
      "text_model.encoder.layers.11.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 11])\n",
      "text_model.encoder.layers.11.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 12])\n",
      "text_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.11.self_attn.v_proj.lora_A\n",
      "torch.Size([5, 768])\n",
      "text_model.encoder.layers.11.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([6, 768])\n",
      "text_model.encoder.layers.11.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 5])\n",
      "text_model.encoder.layers.11.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 6])\n",
      "text_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.2.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.2.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.2.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.2.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.2.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.2.mlp.fc1.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.2.mlp.fc1.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.2.mlp.fc1.lora_B\n",
      "torch.Size([3072, 1])\n",
      "text_model.encoder.layers.2.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 2])\n",
      "text_model.encoder.layers.2.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.2.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.2.mlp.fc2.lora_A\n",
      "torch.Size([1, 3072])\n",
      "text_model.encoder.layers.2.mlp.fc2.lora_A_temp\n",
      "torch.Size([2, 3072])\n",
      "text_model.encoder.layers.2.mlp.fc2.lora_B\n",
      "torch.Size([768, 1])\n",
      "text_model.encoder.layers.2.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.2.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.2.self_attn.k_proj.lora_A\n",
      "torch.Size([14, 768])\n",
      "text_model.encoder.layers.2.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([15, 768])\n",
      "text_model.encoder.layers.2.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 14])\n",
      "text_model.encoder.layers.2.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 15])\n",
      "text_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.2.self_attn.out_proj.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.2.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.2.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 1])\n",
      "text_model.encoder.layers.2.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.2.self_attn.q_proj.lora_A\n",
      "torch.Size([12, 768])\n",
      "text_model.encoder.layers.2.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([13, 768])\n",
      "text_model.encoder.layers.2.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 12])\n",
      "text_model.encoder.layers.2.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 13])\n",
      "text_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.2.self_attn.v_proj.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.2.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.2.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 1])\n",
      "text_model.encoder.layers.2.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.3.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.3.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.3.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.3.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.3.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.3.mlp.fc1.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.3.mlp.fc1.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.3.mlp.fc1.lora_B\n",
      "torch.Size([3072, 1])\n",
      "text_model.encoder.layers.3.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 2])\n",
      "text_model.encoder.layers.3.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.3.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.3.mlp.fc2.lora_A\n",
      "torch.Size([5, 3072])\n",
      "text_model.encoder.layers.3.mlp.fc2.lora_A_temp\n",
      "torch.Size([6, 3072])\n",
      "text_model.encoder.layers.3.mlp.fc2.lora_B\n",
      "torch.Size([768, 5])\n",
      "text_model.encoder.layers.3.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 6])\n",
      "text_model.encoder.layers.3.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.3.self_attn.k_proj.lora_A\n",
      "torch.Size([13, 768])\n",
      "text_model.encoder.layers.3.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([14, 768])\n",
      "text_model.encoder.layers.3.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 13])\n",
      "text_model.encoder.layers.3.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 14])\n",
      "text_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.3.self_attn.out_proj.lora_A\n",
      "torch.Size([18, 768])\n",
      "text_model.encoder.layers.3.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([19, 768])\n",
      "text_model.encoder.layers.3.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 18])\n",
      "text_model.encoder.layers.3.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 19])\n",
      "text_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.3.self_attn.q_proj.lora_A\n",
      "torch.Size([7, 768])\n",
      "text_model.encoder.layers.3.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([8, 768])\n",
      "text_model.encoder.layers.3.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 7])\n",
      "text_model.encoder.layers.3.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 8])\n",
      "text_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.3.self_attn.v_proj.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.3.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.3.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 1])\n",
      "text_model.encoder.layers.3.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.4.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.4.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.4.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.4.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.4.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.4.mlp.fc1.lora_A\n",
      "torch.Size([3, 768])\n",
      "text_model.encoder.layers.4.mlp.fc1.lora_A_temp\n",
      "torch.Size([4, 768])\n",
      "text_model.encoder.layers.4.mlp.fc1.lora_B\n",
      "torch.Size([3072, 3])\n",
      "text_model.encoder.layers.4.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 4])\n",
      "text_model.encoder.layers.4.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.4.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.4.mlp.fc2.lora_A\n",
      "torch.Size([10, 3072])\n",
      "text_model.encoder.layers.4.mlp.fc2.lora_A_temp\n",
      "torch.Size([11, 3072])\n",
      "text_model.encoder.layers.4.mlp.fc2.lora_B\n",
      "torch.Size([768, 10])\n",
      "text_model.encoder.layers.4.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 11])\n",
      "text_model.encoder.layers.4.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.4.self_attn.k_proj.lora_A\n",
      "torch.Size([10, 768])\n",
      "text_model.encoder.layers.4.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([11, 768])\n",
      "text_model.encoder.layers.4.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 10])\n",
      "text_model.encoder.layers.4.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 11])\n",
      "text_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.4.self_attn.out_proj.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.4.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.4.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 1])\n",
      "text_model.encoder.layers.4.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.4.self_attn.q_proj.lora_A\n",
      "torch.Size([12, 768])\n",
      "text_model.encoder.layers.4.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([13, 768])\n",
      "text_model.encoder.layers.4.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 12])\n",
      "text_model.encoder.layers.4.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 13])\n",
      "text_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.4.self_attn.v_proj.lora_A\n",
      "torch.Size([18, 768])\n",
      "text_model.encoder.layers.4.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([19, 768])\n",
      "text_model.encoder.layers.4.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 18])\n",
      "text_model.encoder.layers.4.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 19])\n",
      "text_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.5.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.5.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.5.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.5.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.5.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.5.mlp.fc1.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.5.mlp.fc1.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.5.mlp.fc1.lora_B\n",
      "torch.Size([3072, 1])\n",
      "text_model.encoder.layers.5.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 2])\n",
      "text_model.encoder.layers.5.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.5.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.5.mlp.fc2.lora_A\n",
      "torch.Size([2, 3072])\n",
      "text_model.encoder.layers.5.mlp.fc2.lora_A_temp\n",
      "torch.Size([3, 3072])\n",
      "text_model.encoder.layers.5.mlp.fc2.lora_B\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.5.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 3])\n",
      "text_model.encoder.layers.5.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.5.self_attn.k_proj.lora_A\n",
      "torch.Size([13, 768])\n",
      "text_model.encoder.layers.5.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([14, 768])\n",
      "text_model.encoder.layers.5.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 13])\n",
      "text_model.encoder.layers.5.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 14])\n",
      "text_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.5.self_attn.out_proj.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.5.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.5.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 1])\n",
      "text_model.encoder.layers.5.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.5.self_attn.q_proj.lora_A\n",
      "torch.Size([13, 768])\n",
      "text_model.encoder.layers.5.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([14, 768])\n",
      "text_model.encoder.layers.5.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 13])\n",
      "text_model.encoder.layers.5.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 14])\n",
      "text_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.5.self_attn.v_proj.lora_A\n",
      "torch.Size([18, 768])\n",
      "text_model.encoder.layers.5.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([19, 768])\n",
      "text_model.encoder.layers.5.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 18])\n",
      "text_model.encoder.layers.5.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 19])\n",
      "text_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.6.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.6.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.6.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.6.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.6.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.6.mlp.fc1.lora_A\n",
      "torch.Size([19, 768])\n",
      "text_model.encoder.layers.6.mlp.fc1.lora_A_temp\n",
      "torch.Size([20, 768])\n",
      "text_model.encoder.layers.6.mlp.fc1.lora_B\n",
      "torch.Size([3072, 19])\n",
      "text_model.encoder.layers.6.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 20])\n",
      "text_model.encoder.layers.6.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.6.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.6.mlp.fc2.lora_A\n",
      "torch.Size([4, 3072])\n",
      "text_model.encoder.layers.6.mlp.fc2.lora_A_temp\n",
      "torch.Size([5, 3072])\n",
      "text_model.encoder.layers.6.mlp.fc2.lora_B\n",
      "torch.Size([768, 4])\n",
      "text_model.encoder.layers.6.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 5])\n",
      "text_model.encoder.layers.6.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.6.self_attn.k_proj.lora_A\n",
      "torch.Size([22, 768])\n",
      "text_model.encoder.layers.6.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([23, 768])\n",
      "text_model.encoder.layers.6.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 22])\n",
      "text_model.encoder.layers.6.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 23])\n",
      "text_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.6.self_attn.out_proj.lora_A\n",
      "torch.Size([19, 768])\n",
      "text_model.encoder.layers.6.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([20, 768])\n",
      "text_model.encoder.layers.6.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 19])\n",
      "text_model.encoder.layers.6.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 20])\n",
      "text_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.6.self_attn.q_proj.lora_A\n",
      "torch.Size([16, 768])\n",
      "text_model.encoder.layers.6.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([17, 768])\n",
      "text_model.encoder.layers.6.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 16])\n",
      "text_model.encoder.layers.6.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 17])\n",
      "text_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.6.self_attn.v_proj.lora_A\n",
      "torch.Size([16, 768])\n",
      "text_model.encoder.layers.6.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([17, 768])\n",
      "text_model.encoder.layers.6.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 16])\n",
      "text_model.encoder.layers.6.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 17])\n",
      "text_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.7.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.7.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.7.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.7.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.7.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.7.mlp.fc1.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.7.mlp.fc1.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.7.mlp.fc1.lora_B\n",
      "torch.Size([3072, 1])\n",
      "text_model.encoder.layers.7.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 2])\n",
      "text_model.encoder.layers.7.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.7.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.7.mlp.fc2.lora_A\n",
      "torch.Size([2, 3072])\n",
      "text_model.encoder.layers.7.mlp.fc2.lora_A_temp\n",
      "torch.Size([3, 3072])\n",
      "text_model.encoder.layers.7.mlp.fc2.lora_B\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.7.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 3])\n",
      "text_model.encoder.layers.7.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.7.self_attn.k_proj.lora_A\n",
      "torch.Size([19, 768])\n",
      "text_model.encoder.layers.7.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([20, 768])\n",
      "text_model.encoder.layers.7.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 19])\n",
      "text_model.encoder.layers.7.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 20])\n",
      "text_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.7.self_attn.out_proj.lora_A\n",
      "torch.Size([8, 768])\n",
      "text_model.encoder.layers.7.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([9, 768])\n",
      "text_model.encoder.layers.7.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 8])\n",
      "text_model.encoder.layers.7.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 9])\n",
      "text_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.7.self_attn.q_proj.lora_A\n",
      "torch.Size([20, 768])\n",
      "text_model.encoder.layers.7.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([21, 768])\n",
      "text_model.encoder.layers.7.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 20])\n",
      "text_model.encoder.layers.7.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 21])\n",
      "text_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.7.self_attn.v_proj.lora_A\n",
      "torch.Size([11, 768])\n",
      "text_model.encoder.layers.7.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([12, 768])\n",
      "text_model.encoder.layers.7.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 11])\n",
      "text_model.encoder.layers.7.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 12])\n",
      "text_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.8.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.8.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.8.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.8.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.8.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.8.mlp.fc1.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.8.mlp.fc1.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.8.mlp.fc1.lora_B\n",
      "torch.Size([3072, 1])\n",
      "text_model.encoder.layers.8.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 2])\n",
      "text_model.encoder.layers.8.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.8.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.8.mlp.fc2.lora_A\n",
      "torch.Size([2, 3072])\n",
      "text_model.encoder.layers.8.mlp.fc2.lora_A_temp\n",
      "torch.Size([3, 3072])\n",
      "text_model.encoder.layers.8.mlp.fc2.lora_B\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.8.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 3])\n",
      "text_model.encoder.layers.8.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.8.self_attn.k_proj.lora_A\n",
      "torch.Size([1, 768])\n",
      "text_model.encoder.layers.8.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([2, 768])\n",
      "text_model.encoder.layers.8.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 1])\n",
      "text_model.encoder.layers.8.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.8.self_attn.out_proj.lora_A\n",
      "torch.Size([7, 768])\n",
      "text_model.encoder.layers.8.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([8, 768])\n",
      "text_model.encoder.layers.8.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 7])\n",
      "text_model.encoder.layers.8.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 8])\n",
      "text_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.8.self_attn.q_proj.lora_A\n",
      "torch.Size([13, 768])\n",
      "text_model.encoder.layers.8.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([14, 768])\n",
      "text_model.encoder.layers.8.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 13])\n",
      "text_model.encoder.layers.8.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 14])\n",
      "text_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.8.self_attn.v_proj.lora_A\n",
      "torch.Size([10, 768])\n",
      "text_model.encoder.layers.8.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([11, 768])\n",
      "text_model.encoder.layers.8.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 10])\n",
      "text_model.encoder.layers.8.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 11])\n",
      "text_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.9.layer_norm1.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.9.layer_norm1.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.9.layer_norm2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.9.layer_norm2.weight\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.9.mlp.fc1.bias\n",
      "torch.Size([3072])\n",
      "text_model.encoder.layers.9.mlp.fc1.lora_A\n",
      "torch.Size([3, 768])\n",
      "text_model.encoder.layers.9.mlp.fc1.lora_A_temp\n",
      "torch.Size([4, 768])\n",
      "text_model.encoder.layers.9.mlp.fc1.lora_B\n",
      "torch.Size([3072, 3])\n",
      "text_model.encoder.layers.9.mlp.fc1.lora_B_temp\n",
      "torch.Size([3072, 4])\n",
      "text_model.encoder.layers.9.mlp.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "text_model.encoder.layers.9.mlp.fc2.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.9.mlp.fc2.lora_A\n",
      "torch.Size([2, 3072])\n",
      "text_model.encoder.layers.9.mlp.fc2.lora_A_temp\n",
      "torch.Size([3, 3072])\n",
      "text_model.encoder.layers.9.mlp.fc2.lora_B\n",
      "torch.Size([768, 2])\n",
      "text_model.encoder.layers.9.mlp.fc2.lora_B_temp\n",
      "torch.Size([768, 3])\n",
      "text_model.encoder.layers.9.mlp.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "text_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.9.self_attn.k_proj.lora_A\n",
      "torch.Size([16, 768])\n",
      "text_model.encoder.layers.9.self_attn.k_proj.lora_A_temp\n",
      "torch.Size([17, 768])\n",
      "text_model.encoder.layers.9.self_attn.k_proj.lora_B\n",
      "torch.Size([768, 16])\n",
      "text_model.encoder.layers.9.self_attn.k_proj.lora_B_temp\n",
      "torch.Size([768, 17])\n",
      "text_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.9.self_attn.out_proj.lora_A\n",
      "torch.Size([4, 768])\n",
      "text_model.encoder.layers.9.self_attn.out_proj.lora_A_temp\n",
      "torch.Size([5, 768])\n",
      "text_model.encoder.layers.9.self_attn.out_proj.lora_B\n",
      "torch.Size([768, 4])\n",
      "text_model.encoder.layers.9.self_attn.out_proj.lora_B_temp\n",
      "torch.Size([768, 5])\n",
      "text_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.9.self_attn.q_proj.lora_A\n",
      "torch.Size([7, 768])\n",
      "text_model.encoder.layers.9.self_attn.q_proj.lora_A_temp\n",
      "torch.Size([8, 768])\n",
      "text_model.encoder.layers.9.self_attn.q_proj.lora_B\n",
      "torch.Size([768, 7])\n",
      "text_model.encoder.layers.9.self_attn.q_proj.lora_B_temp\n",
      "torch.Size([768, 8])\n",
      "text_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "text_model.encoder.layers.9.self_attn.v_proj.lora_A\n",
      "torch.Size([4, 768])\n",
      "text_model.encoder.layers.9.self_attn.v_proj.lora_A_temp\n",
      "torch.Size([5, 768])\n",
      "text_model.encoder.layers.9.self_attn.v_proj.lora_B\n",
      "torch.Size([768, 4])\n",
      "text_model.encoder.layers.9.self_attn.v_proj.lora_B_temp\n",
      "torch.Size([768, 5])\n",
      "text_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "text_model.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "text_model.final_layer_norm.weight\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, tensor in model_text.items():\n",
    "    print(name)\n",
    "    print(tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sel_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
